{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "v4l5hTRx7fT5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Hello World from Sue"
      ]
    },
    {
      "metadata": {
        "id": "pbOlqPp_7bo0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AJILcbvN7n85",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "helloworld=tf.constant(\"Hello world.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r4iU8FHT7vjr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "a45adbc0-a932-4679-dd96-167f22a19691"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  print(sess.run(helloworld))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Hello world.'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uAhOZrcL78x5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Add and Multiply"
      ]
    },
    {
      "metadata": {
        "id": "GwD7m68r75un",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a=tf.constant(2)\n",
        "b=tf.constant(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J_Yv41sR8m-0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "c768b7ce-c896-460c-ed04-fa1b573e035a"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  print(sess.run(a+b))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xwxw5TlI8Pab",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "add=tf.add(a,b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "reeqVNFG8Eis",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "f539b1f0-a673-434f-d8b4-af49e9dc6d96"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  print(sess.run(add,feed_dict={a:2,b:3}))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zjWrYQOm8NTy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "cc8e3fb8-9bca-4180-c72a-0e69489546b6"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  print(sess.run(a*b))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jlgKw5U089vX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mul=tf.multiply(a,b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aTdD0kYH84qb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "ccbc26bd-5103-40a4-bb2f-6e99d91bd69a"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  print(sess.run(mul,feed_dict={a:2,b:3}))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HlGWJ1RJ9r3v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "linear regression"
      ]
    },
    {
      "metadata": {
        "id": "rVup8kz69Hrp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C3enSTl0_squ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate=0.1\n",
        "training_epochs=200\n",
        "display_step=20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xhFDRZSrAGGh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_X=np.asarray([2,3,5,3,4,5,3,5,7,8,4,2,5,8,5])\n",
        "train_Y=np.asarray([2,3,5,3,5,6,7,5,6,7,8,9,6,5,4])\n",
        "n_samples=train_X.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aacgU8_NAhR5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "6acbbed9-32fd-4f15-831b-eee3bd08f4ca"
      },
      "cell_type": "code",
      "source": [
        "X=tf.placeholder(float)\n",
        "Y=tf.placeholder(float)\n",
        "W=tf.Variable(np.random.randn(),name='weight')\n",
        "b=tf.Variable(np.random.randn(),name='bias')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cxsb_YUzBE_E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred_y=tf.add(tf.multiply(W,X),b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "34BLjgrlBWf2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cost = tf.reduce_sum(tf.pow(pred_y-Y, 2))/(2*n_samples)\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "10Tiww44BdkT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init=tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OL0qLVNZB07S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "outputId": "70ad230d-91da-46e1-ad69-17414ccd9ff0"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "\n",
        "    for epoch in range(training_epochs):\n",
        "        for (x, y) in zip(train_X, train_Y):\n",
        "            sess.run(optimizer, feed_dict={X: x, Y: y})\n",
        "\n",
        "        if (epoch+1) % display_step == 0:\n",
        "            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})\n",
        "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \\\n",
        "                \"W=\", sess.run(W), \"b=\", sess.run(b))\n",
        "\n",
        "    print (\"Optimization Finished!\")\n",
        "    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\n",
        "    print (\"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\n",
        "\n",
        "    plt.plot(train_X, train_Y, 'ro', label='Original data')\n",
        "    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0020 cost= 2.908890963 W= 0.7959071 b= 0.6947646\n",
            "Epoch: 0040 cost= 2.346056223 W= 0.6207592 b= 1.7559736\n",
            "Epoch: 0060 cost= 2.034725666 W= 0.48874208 b= 2.555856\n",
            "Epoch: 0080 cost= 1.864205480 W= 0.3892346 b= 3.1587644\n",
            "Epoch: 0100 cost= 1.772119403 W= 0.31423143 b= 3.6132038\n",
            "Epoch: 0120 cost= 1.723413944 W= 0.25769782 b= 3.9557366\n",
            "Epoch: 0140 cost= 1.698465228 W= 0.2150861 b= 4.213918\n",
            "Epoch: 0160 cost= 1.686342955 W= 0.18296744 b= 4.408523\n",
            "Epoch: 0180 cost= 1.681002498 W= 0.15875816 b= 4.5552053\n",
            "Epoch: 0200 cost= 1.679134488 W= 0.14051081 b= 4.665765\n",
            "Optimization Finished!\n",
            "Training cost= 1.6791345 W= 0.14051081 b= 4.665765 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdkAAAFKCAYAAABRtSXvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "6H25dU5AFRHz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "logistic regression"
      ]
    },
    {
      "metadata": {
        "id": "ZZY80na5B_El",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "3ff37388-e635-4584-a0d6-476e881603a3"
      },
      "cell_type": "code",
      "source": [
        "# Import MINST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"official/mnist/dataset.py\", one_hot=True)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting official/mnist/dataset.py/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting official/mnist/dataset.py/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting official/mnist/dataset.py/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting official/mnist/dataset.py/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fytA4JRyFU_P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "learning_rate = 0.006\n",
        "training_epochs = 66\n",
        "batch_size = 30\n",
        "display_step = 1\n",
        "\n",
        "# tf Graph Input\n",
        "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
        "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
        "\n",
        "# Set model weights\n",
        "W = tf.Variable(tf.zeros([784, 10]))\n",
        "b = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "# Construct model\n",
        "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
        "\n",
        "# Minimize error using cross entropy\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
        "# Gradient Descent\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CA88Y6ZiGDkV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1150
        },
        "outputId": "94a481b6-11d7-45c3-9825-fab8187016f8"
      },
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "\n",
        "    # Training cycle\n",
        "    for epoch in range(training_epochs):\n",
        "        avg_cost = 0.\n",
        "        total_batch = int(mnist.train.num_examples/batch_size)\n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "            # Fit training using batch data\n",
        "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
        "                                                          y: batch_ys})\n",
        "            # Compute average loss\n",
        "            avg_cost += c / total_batch\n",
        "        # Display logs per epoch step\n",
        "        if (epoch+1) % display_step == 0:\n",
        "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
        "\n",
        "    print (\"Optimization Finished!\")\n",
        "\n",
        "    # Test model\n",
        "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "    # Calculate accuracy for 3000 examples\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    print (\"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost= 0.926269892\n",
            "Epoch: 0002 cost= 0.527171369\n",
            "Epoch: 0003 cost= 0.454098660\n",
            "Epoch: 0004 cost= 0.419242476\n",
            "Epoch: 0005 cost= 0.397389584\n",
            "Epoch: 0006 cost= 0.381052485\n",
            "Epoch: 0007 cost= 0.370503484\n",
            "Epoch: 0008 cost= 0.360662854\n",
            "Epoch: 0009 cost= 0.354834970\n",
            "Epoch: 0010 cost= 0.347146955\n",
            "Epoch: 0011 cost= 0.340402213\n",
            "Epoch: 0012 cost= 0.338258643\n",
            "Epoch: 0013 cost= 0.333304418\n",
            "Epoch: 0014 cost= 0.330521983\n",
            "Epoch: 0015 cost= 0.324501880\n",
            "Epoch: 0016 cost= 0.324268317\n",
            "Epoch: 0017 cost= 0.319949694\n",
            "Epoch: 0018 cost= 0.317689955\n",
            "Epoch: 0019 cost= 0.316941556\n",
            "Epoch: 0020 cost= 0.313266457\n",
            "Epoch: 0021 cost= 0.312367705\n",
            "Epoch: 0022 cost= 0.309641012\n",
            "Epoch: 0023 cost= 0.308448958\n",
            "Epoch: 0024 cost= 0.306908780\n",
            "Epoch: 0025 cost= 0.304593156\n",
            "Epoch: 0026 cost= 0.304762499\n",
            "Epoch: 0027 cost= 0.301462357\n",
            "Epoch: 0028 cost= 0.301112141\n",
            "Epoch: 0029 cost= 0.299973942\n",
            "Epoch: 0030 cost= 0.300506853\n",
            "Epoch: 0031 cost= 0.297409770\n",
            "Epoch: 0032 cost= 0.296146560\n",
            "Epoch: 0033 cost= 0.295239146\n",
            "Epoch: 0034 cost= 0.294943273\n",
            "Epoch: 0035 cost= 0.293472938\n",
            "Epoch: 0036 cost= 0.294452500\n",
            "Epoch: 0037 cost= 0.292828482\n",
            "Epoch: 0038 cost= 0.290421204\n",
            "Epoch: 0039 cost= 0.291324028\n",
            "Epoch: 0040 cost= 0.290368269\n",
            "Epoch: 0041 cost= 0.288094701\n",
            "Epoch: 0042 cost= 0.288753463\n",
            "Epoch: 0043 cost= 0.287987495\n",
            "Epoch: 0044 cost= 0.286666169\n",
            "Epoch: 0045 cost= 0.287628829\n",
            "Epoch: 0046 cost= 0.286280447\n",
            "Epoch: 0047 cost= 0.285328586\n",
            "Epoch: 0048 cost= 0.284462576\n",
            "Epoch: 0049 cost= 0.284525909\n",
            "Epoch: 0050 cost= 0.283861132\n",
            "Epoch: 0051 cost= 0.282813512\n",
            "Epoch: 0052 cost= 0.282351879\n",
            "Epoch: 0053 cost= 0.281897442\n",
            "Epoch: 0054 cost= 0.282410039\n",
            "Epoch: 0055 cost= 0.280813488\n",
            "Epoch: 0056 cost= 0.281060073\n",
            "Epoch: 0057 cost= 0.279432696\n",
            "Epoch: 0058 cost= 0.281197637\n",
            "Epoch: 0059 cost= 0.278764357\n",
            "Epoch: 0060 cost= 0.280255090\n",
            "Epoch: 0061 cost= 0.277782853\n",
            "Epoch: 0062 cost= 0.278495265\n",
            "Epoch: 0063 cost= 0.277561025\n",
            "Epoch: 0064 cost= 0.277634403\n",
            "Epoch: 0065 cost= 0.276897944\n",
            "Epoch: 0066 cost= 0.275772201\n",
            "Optimization Finished!\n",
            "Accuracy: 0.898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z5yjfXqNJT4q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "word2vec"
      ]
    },
    {
      "metadata": {
        "id": "oFOrUKBIGK_9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import division, print_function, absolute_import\n",
        "\n",
        "import collections\n",
        "import os\n",
        "import random\n",
        "import urllib\n",
        "import zipfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VzOJSP2aJVd_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training Parameters\n",
        "learning_rate = 0.1\n",
        "batch_size = 128\n",
        "num_steps = 3000000\n",
        "display_step = 10000\n",
        "eval_step = 200000\n",
        "\n",
        "# Evaluation Parameters\n",
        "eval_words = [b\"science\", b\"physics\", b\"finance\", b\"language\", b\"eat\", b\"happy\"]\n",
        "\n",
        "# Word2Vec Parameters\n",
        "embedding_size = 200 # Dimension of the embedding vector\n",
        "max_vocabulary_size = 50000 # Total number of different words in the vocabulary\n",
        "min_occurrence = 10 # Remove all words that does not appears at least n times\n",
        "skip_window = 3 # How many words to consider left and right\n",
        "num_skips = 2 # How many times to reuse an input to generate a label\n",
        "num_sampled = 64 # Number of negative examples to sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6JA9BLDHJfXf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download a small chunk of Wikipedia articles collection\n",
        "import urllib.request\n",
        "url ='http://mattmahoney.net/dc/text8.zip'\n",
        "data_path='text8.zip'\n",
        "if not os.path.exists(data_path):\n",
        "    print(\"Downloading the dataset... (It may take some time)\")\n",
        "    filename, _ = urllib.request.urlretrieve(url, data_path)\n",
        "    print(\"Done!\")\n",
        "# Unzip the dataset file. Text has already been processed\n",
        "with zipfile.ZipFile(data_path) as f:\n",
        "    text_words = f.read(f.namelist()[0]).lower().split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z5y4lXTjJiE1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "f62b382e-aa70-4204-ae1f-54bfaa7fc6ad"
      },
      "cell_type": "code",
      "source": [
        "# Build the dictionary and replace rare words with UNK token\n",
        "count = [('UNK', -1)]\n",
        "# Retrieve the most common words\n",
        "count.extend(collections.Counter(text_words).most_common(max_vocabulary_size - 1))\n",
        "# Remove samples with less than 'min_occurrence' occurrences\n",
        "for i in range(len(count) - 1, -1, -1):\n",
        "    if count[i][1] < min_occurrence:\n",
        "        count.pop(i)\n",
        "    else:\n",
        "        # The collection is ordered, so stop when 'min_occurrence' is reached\n",
        "        break\n",
        "# Compute the vocabulary size\n",
        "vocabulary_size = len(count)\n",
        "# Assign an id to each word\n",
        "word2id = dict()\n",
        "for i, (word, _)in enumerate(count):\n",
        "    word2id[word] = i\n",
        "\n",
        "data = list()\n",
        "unk_count = 0\n",
        "for word in text_words:\n",
        "    # Retrieve a word id, or assign it index 0 ('UNK') if not in dictionary\n",
        "    index = word2id.get(word, 0)\n",
        "    if index == 0:\n",
        "        unk_count += 1\n",
        "    data.append(index)\n",
        "count[0] = ('UNK', unk_count)\n",
        "id2word = dict(zip(word2id.values(), word2id.keys()))\n",
        "\n",
        "print(\"Words count:\", len(text_words))\n",
        "print(\"Unique words:\", len(set(text_words)))\n",
        "print(\"Vocabulary size:\", vocabulary_size)\n",
        "print(\"Most common words:\", count[:10])"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count: 17005207\n",
            "Unique words: 253854\n",
            "Vocabulary size: 47135\n",
            "Most common words: [('UNK', 444176), (b'the', 1061396), (b'of', 593677), (b'and', 416629), (b'one', 411764), (b'in', 372201), (b'a', 325873), (b'to', 316376), (b'zero', 264975), (b'nine', 250430)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gTawoaTeK9ou",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_index = 0\n",
        "# Generate training batch for the skip-gram model\n",
        "def next_batch(batch_size, num_skips, skip_window):\n",
        "    global data_index\n",
        "    assert batch_size % num_skips == 0\n",
        "    assert num_skips <= 2 * skip_window\n",
        "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "    # get window size (words left and right + current one)\n",
        "    span = 2 * skip_window + 1\n",
        "    buffer = collections.deque(maxlen=span)\n",
        "    if data_index + span > len(data):\n",
        "        data_index = 0\n",
        "    buffer.extend(data[data_index:data_index + span])\n",
        "    data_index += span\n",
        "    for i in range(batch_size // num_skips):\n",
        "        context_words = [w for w in range(span) if w != skip_window]\n",
        "        words_to_use = random.sample(context_words, num_skips)\n",
        "        for j, context_word in enumerate(words_to_use):\n",
        "            batch[i * num_skips + j] = buffer[skip_window]\n",
        "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
        "        if data_index == len(data):\n",
        "            buffer.extend(data[0:span])\n",
        "            data_index = span\n",
        "        else:\n",
        "            buffer.append(data[data_index])\n",
        "            data_index += 1\n",
        "    data_index = (data_index + len(data) - span) % len(data)\n",
        "    return batch, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1G-2-2OTMBdI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Input data\n",
        "X = tf.placeholder(tf.int32, shape=[None])\n",
        "# Input label\n",
        "Y = tf.placeholder(tf.int32, shape=[None, 1])\n",
        "\n",
        "# Ensure the following ops & var are assigned on CPU\n",
        "# (some ops are not compatible on GPU)\n",
        "with tf.device('/cpu:0'):\n",
        "    # Create the embedding variable (each row represent a word embedding vector)\n",
        "    embedding = tf.Variable(tf.random_normal([vocabulary_size, embedding_size]))\n",
        "    # Lookup the corresponding embedding vectors for each sample in X\n",
        "    X_embed = tf.nn.embedding_lookup(embedding, X)\n",
        "\n",
        "    # Construct the variables for the NCE loss\n",
        "    nce_weights = tf.Variable(tf.random_normal([vocabulary_size, embedding_size]))\n",
        "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
        "\n",
        "# Compute the average NCE loss for the batch\n",
        "loss_op = tf.reduce_mean(\n",
        "    tf.nn.nce_loss(weights=nce_weights,\n",
        "                   biases=nce_biases,\n",
        "                   labels=Y,\n",
        "                   inputs=X_embed,\n",
        "                   num_sampled=num_sampled,\n",
        "                   num_classes=vocabulary_size))\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Evaluation\n",
        "# Compute the cosine similarity between input data embedding and every embedding vectors\n",
        "X_embed_norm = X_embed / tf.sqrt(tf.reduce_sum(tf.square(X_embed)))\n",
        "embedding_norm = embedding / tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keepdims=True))\n",
        "cosine_sim_op = tf.matmul(X_embed_norm, embedding_norm, transpose_b=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fo8Br3mHMJQo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4733
        },
        "outputId": "e5cf2bc2-118a-4447-9d8b-75e1d1b58376"
      },
      "cell_type": "code",
      "source": [
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "\n",
        "    # Testing data\n",
        "    x_test = np.array([word2id[w] for w in eval_words])\n",
        "\n",
        "    average_loss = 0\n",
        "    for step in range(1, num_steps + 1):\n",
        "        # Get a new batch of data\n",
        "        batch_x, batch_y = next_batch(batch_size, num_skips, skip_window)\n",
        "        # Run training op\n",
        "        _, loss = sess.run([train_op, loss_op], feed_dict={X: batch_x, Y: batch_y})\n",
        "        average_loss += loss\n",
        "\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            if step > 1:\n",
        "                average_loss /= display_step\n",
        "            print(\"Step \" + str(step) + \", Average Loss= \" + \\\n",
        "                  \"{:.4f}\".format(average_loss))\n",
        "            average_loss = 0   \n",
        "\n",
        "        # Evaluation\n",
        "        if step % eval_step == 0 or step == 1:\n",
        "            print(\"Evaluation...\")\n",
        "            sim = sess.run(cosine_sim_op, feed_dict={X: x_test})\n",
        "            for i in range(len(eval_words)):\n",
        "                top_k = 8  # number of nearest neighbors\n",
        "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
        "                log_str = '\"%s\" nearest neighbors:' % eval_words[i]\n",
        "                for k in range(top_k):\n",
        "                    log_str = '%s %s,' % (log_str, id2word[nearest[k]])\n",
        "                print(log_str)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1, Average Loss= 541.2406\n",
            "Evaluation...\n",
            "\"b'science'\" nearest neighbors: b'advocate', b'pitt', b'salutation', b'scuderia', b'harassment', b'sagittarius', b'comply', b'trouble',\n",
            "\"b'physics'\" nearest neighbors: b'hplc', b'sieve', b'watched', b'befriended', b'veneto', b'getz', b'packets', b'bracing',\n",
            "\"b'finance'\" nearest neighbors: b'muster', b'reactive', b'tao', b'reworking', b'gerrard', b'lieder', b'auspices', b'carries',\n",
            "\"b'language'\" nearest neighbors: b'landmass', b'kline', b'entire', b'encountering', b'omnivorous', b'nottingham', b'neq', b'wonderswan',\n",
            "\"b'eat'\" nearest neighbors: b'when', b'infidels', b'quarters', b'vases', b'swampy', b'premiums', b'rojcewicz', b'hadiths',\n",
            "\"b'happy'\" nearest neighbors: b'ran', b'counteract', b'kavina', b'demi', b'bannister', b'grosso', b'growth', b'outfielder',\n",
            "Step 10000, Average Loss= 201.0397\n",
            "Step 20000, Average Loss= 95.8515\n",
            "Step 30000, Average Loss= 64.9165\n",
            "Step 40000, Average Loss= 49.8813\n",
            "Step 50000, Average Loss= 41.9771\n",
            "Step 60000, Average Loss= 35.5218\n",
            "Step 70000, Average Loss= 32.7561\n",
            "Step 80000, Average Loss= 28.3001\n",
            "Step 90000, Average Loss= 26.6505\n",
            "Step 100000, Average Loss= 24.4567\n",
            "Step 110000, Average Loss= 22.5843\n",
            "Step 120000, Average Loss= 20.8949\n",
            "Step 130000, Average Loss= 19.4563\n",
            "Step 140000, Average Loss= 19.5361\n",
            "Step 150000, Average Loss= 18.1564\n",
            "Step 160000, Average Loss= 18.4985\n",
            "Step 170000, Average Loss= 16.4562\n",
            "Step 180000, Average Loss= 16.1492\n",
            "Step 190000, Average Loss= 16.0119\n",
            "Step 200000, Average Loss= 15.1013\n",
            "Evaluation...\n",
            "\"b'science'\" nearest neighbors: b'with', b'while', b'has', b'all', b'on', b'use', b'from', b'are',\n",
            "\"b'physics'\" nearest neighbors: b'both', b'history', b'at', UNK, b's', b'from', b'british', b'when',\n",
            "\"b'finance'\" nearest neighbors: b'texts', b'activities', b'local', b'produce', b'border', b'equal', b'december', b'victims',\n",
            "\"b'language'\" nearest neighbors: b'often', b'called', b'other', b'they', b'some', b'time', b'to', b'if',\n",
            "\"b'eat'\" nearest neighbors: b'presence', b'council', b'books', b'inside', b'earlier', b'episodes', b'moral', b'page',\n",
            "\"b'happy'\" nearest neighbors: b'islands', b'generation', b'growth', b'ran', b'ideas', b'reported', b'academy', b'flag',\n",
            "Step 210000, Average Loss= 14.9590\n",
            "Step 220000, Average Loss= 14.7240\n",
            "Step 230000, Average Loss= 13.7913\n",
            "Step 240000, Average Loss= 13.5402\n",
            "Step 250000, Average Loss= 13.1748\n",
            "Step 260000, Average Loss= 12.7395\n",
            "Step 270000, Average Loss= 12.5394\n",
            "Step 280000, Average Loss= 11.9109\n",
            "Step 290000, Average Loss= 11.5077\n",
            "Step 300000, Average Loss= 11.4647\n",
            "Step 310000, Average Loss= 10.7987\n",
            "Step 320000, Average Loss= 10.8029\n",
            "Step 330000, Average Loss= 10.8981\n",
            "Step 340000, Average Loss= 10.2846\n",
            "Step 350000, Average Loss= 10.5440\n",
            "Step 360000, Average Loss= 10.1228\n",
            "Step 370000, Average Loss= 10.1362\n",
            "Step 380000, Average Loss= 9.6282\n",
            "Step 390000, Average Loss= 9.5249\n",
            "Step 400000, Average Loss= 9.4915\n",
            "Evaluation...\n",
            "\"b'science'\" nearest neighbors: b'high', b'a', b'other', b'from', b'such', b'including', b'language', b'where',\n",
            "\"b'physics'\" nearest neighbors: b'on', b'both', b'for', b'with', b'second', b'each', b'under', b'when',\n",
            "\"b'finance'\" nearest neighbors: b'there', b'still', b'used', b'known', b'texts', b'them', b'form', b'with',\n",
            "\"b'language'\" nearest neighbors: b'such', b'being', b'this', b'while', b'which', b'only', b'use', b'number',\n",
            "\"b'eat'\" nearest neighbors: b'among', b'presence', b'council', b'known', b'episodes', b'such', UNK, b'important',\n",
            "\"b'happy'\" nearest neighbors: b'world', b'ran', b'war', b'example', b'name', b'called', b'de', b'between',\n",
            "Step 410000, Average Loss= 9.7989\n",
            "Step 420000, Average Loss= 9.3720\n",
            "Step 430000, Average Loss= 9.4845\n",
            "Step 440000, Average Loss= 9.0342\n",
            "Step 450000, Average Loss= 9.2540\n",
            "Step 460000, Average Loss= 9.1735\n",
            "Step 470000, Average Loss= 9.0747\n",
            "Step 480000, Average Loss= 8.8779\n",
            "Step 490000, Average Loss= 9.0453\n",
            "Step 500000, Average Loss= 8.6496\n",
            "Step 510000, Average Loss= 8.7535\n",
            "Step 520000, Average Loss= 8.4958\n",
            "Step 530000, Average Loss= 8.5527\n",
            "Step 540000, Average Loss= 8.6090\n",
            "Step 550000, Average Loss= 8.2946\n",
            "Step 560000, Average Loss= 8.2473\n",
            "Step 570000, Average Loss= 8.0737\n",
            "Step 580000, Average Loss= 8.0692\n",
            "Step 590000, Average Loss= 7.9525\n",
            "Step 600000, Average Loss= 8.1549\n",
            "Evaluation...\n",
            "\"b'science'\" nearest neighbors: b'using', b'based', b'modern', b'include', b'where', b'large', b'including', b'other',\n",
            "\"b'physics'\" nearest neighbors: b'he', b'on', b'including', b'back', b'city', b'and', b'in', b'his',\n",
            "\"b'finance'\" nearest neighbors: b'or', b'single', b'is', b'which', b'than', b'own', b'by', b'made',\n",
            "\"b'language'\" nearest neighbors: b'being', b'form', b'its', b'usually', b'for', b'which', b'use', b'many',\n",
            "\"b'eat'\" nearest neighbors: b'among', b'many', b'other', b'language', b'such', b'well', b'who', b'some',\n",
            "\"b'happy'\" nearest neighbors: b'world', b'ii', b'de', b'with', b'example', b'was', b'being', b'as',\n",
            "Step 610000, Average Loss= 7.7725\n",
            "Step 620000, Average Loss= 7.9393\n",
            "Step 630000, Average Loss= 7.8265\n",
            "Step 640000, Average Loss= 7.7522\n",
            "Step 650000, Average Loss= 7.6554\n",
            "Step 660000, Average Loss= 7.3583\n",
            "Step 670000, Average Loss= 7.6283\n",
            "Step 680000, Average Loss= 7.6255\n",
            "Step 690000, Average Loss= 7.7632\n",
            "Step 700000, Average Loss= 7.4330\n",
            "Step 710000, Average Loss= 7.5246\n",
            "Step 720000, Average Loss= 7.5871\n",
            "Step 730000, Average Loss= 7.4212\n",
            "Step 740000, Average Loss= 7.4203\n",
            "Step 750000, Average Loss= 7.4353\n",
            "Step 760000, Average Loss= 7.3680\n",
            "Step 770000, Average Loss= 7.2778\n",
            "Step 780000, Average Loss= 7.3755\n",
            "Step 790000, Average Loss= 7.2369\n",
            "Step 800000, Average Loss= 7.3089\n",
            "Evaluation...\n",
            "\"b'science'\" nearest neighbors: b'include', b'of', b'including', b'various', b'based', b'or', b'other', b'see',\n",
            "\"b'physics'\" nearest neighbors: b'most', b'back', b'life', b'european', b'water', b'modern', b'some', b'using',\n",
            "\"b'finance'\" nearest neighbors: b'single', b'these', b'major', b'there', b'however', b'are', b'them', b'without',\n",
            "\"b'language'\" nearest neighbors: b'use', b'other', b'also', b'such', b'though', b'which', b'many', b'include',\n",
            "\"b'eat'\" nearest neighbors: b'language', b'among', b'languages', b'other', b'many', b'even', b'like', b'use',\n",
            "\"b'happy'\" nearest neighbors: b'used', b'due', b'world', b'known', b'with', b'back', b'law', b'through',\n",
            "Step 810000, Average Loss= 7.2537\n",
            "Step 820000, Average Loss= 7.1289\n",
            "Step 830000, Average Loss= 7.1408\n",
            "Step 840000, Average Loss= 7.0085\n",
            "Step 850000, Average Loss= 7.0468\n",
            "Step 860000, Average Loss= 6.9739\n",
            "Step 870000, Average Loss= 7.0622\n",
            "Step 880000, Average Loss= 6.9950\n",
            "Step 890000, Average Loss= 6.9653\n",
            "Step 900000, Average Loss= 6.9559\n",
            "Step 910000, Average Loss= 6.8546\n",
            "Step 920000, Average Loss= 6.7105\n",
            "Step 930000, Average Loss= 6.6778\n",
            "Step 940000, Average Loss= 6.9575\n",
            "Step 950000, Average Loss= 6.7728\n",
            "Step 960000, Average Loss= 6.9439\n",
            "Step 970000, Average Loss= 6.6452\n",
            "Step 980000, Average Loss= 6.7709\n",
            "Step 990000, Average Loss= 6.8587\n",
            "Step 1000000, Average Loss= 6.7603\n",
            "Evaluation...\n",
            "\"b'science'\" nearest neighbors: b'various', b'including', b'history', b'modern', b'based', b'include', b'with', b'on',\n",
            "\"b'physics'\" nearest neighbors: b'modern', b'from', b'various', b'life', b'history', b'further', b'on', b'water',\n",
            "\"b'finance'\" nearest neighbors: b'single', b'left', b'right', b'common', b'down', b'major', b'both', b'more',\n",
            "\"b'language'\" nearest neighbors: b'modern', b'example', b'for', b'most', b'such', b'other', b'its', b'also',\n",
            "\"b'eat'\" nearest neighbors: b'among', b'languages', b'called', b'these', b'for', b'family', b'usually', b'language',\n",
            "\"b'happy'\" nearest neighbors: b'religious', b'due', b'world', b'as', b'known', b'such', b'the', b'order',\n",
            "Step 1010000, Average Loss= 6.6619\n",
            "Step 1020000, Average Loss= 6.7951\n",
            "Step 1030000, Average Loss= 6.6624\n",
            "Step 1040000, Average Loss= 6.7209\n",
            "Step 1050000, Average Loss= 6.6600\n",
            "Step 1060000, Average Loss= 6.6759\n",
            "Step 1070000, Average Loss= 6.7196\n",
            "Step 1080000, Average Loss= 6.6424\n",
            "Step 1090000, Average Loss= 6.6018\n",
            "Step 1100000, Average Loss= 6.6095\n",
            "Step 1110000, Average Loss= 6.5393\n",
            "Step 1120000, Average Loss= 6.5056\n",
            "Step 1130000, Average Loss= 6.5878\n",
            "Step 1140000, Average Loss= 6.4303\n",
            "Step 1150000, Average Loss= 6.5536\n",
            "Step 1160000, Average Loss= 6.4629\n",
            "Step 1170000, Average Loss= 6.5132\n",
            "Step 1180000, Average Loss= 6.3876\n",
            "Step 1190000, Average Loss= 6.1858\n",
            "Step 1200000, Average Loss= 6.3647\n",
            "Evaluation...\n",
            "\"b'science'\" nearest neighbors: b'various', b'modern', b'including', b'list', b'based', b'development', b'include', b'other',\n",
            "\"b'physics'\" nearest neighbors: b'modern', b'both', b'each', b'further', b'including', b'some', b'most', b'for',\n",
            "\"b'finance'\" nearest neighbors: b'major', b'left', b'single', b'an', b'still', b'different', b'this', b'down',\n",
            "\"b'language'\" nearest neighbors: b'modern', b'most', b'include', b'other', b'various', b'including', b'example', b'form',\n",
            "\"b'eat'\" nearest neighbors: b'languages', b'among', b'like', b'today', b'short', b'including', b'important', b'many',\n",
            "\"b'happy'\" nearest neighbors: b'religious', b'due', b'known', b'military', b'second', b'played', b'major', b'their',\n",
            "Step 1210000, Average Loss= 6.4999\n",
            "Step 1220000, Average Loss= 6.4640\n",
            "Step 1230000, Average Loss= 6.3508\n",
            "Step 1240000, Average Loss= 6.3864\n",
            "Step 1250000, Average Loss= 6.4128\n",
            "Step 1260000, Average Loss= 6.4146\n",
            "Step 1270000, Average Loss= 6.3167\n",
            "Step 1280000, Average Loss= 6.3685\n",
            "Step 1290000, Average Loss= 6.3529\n",
            "Step 1300000, Average Loss= 6.3472\n",
            "Step 1310000, Average Loss= 6.3977\n",
            "Step 1320000, Average Loss= 6.2835\n",
            "Step 1330000, Average Loss= 6.3510\n",
            "Step 1340000, Average Loss= 6.3896\n",
            "Step 1350000, Average Loss= 6.2871\n",
            "Step 1360000, Average Loss= 6.3131\n",
            "Step 1370000, Average Loss= 6.2234\n",
            "Step 1380000, Average Loss= 6.2672\n",
            "Step 1390000, Average Loss= 6.1883\n",
            "Step 1400000, Average Loss= 6.2992\n",
            "Evaluation...\n",
            "\"b'science'\" nearest neighbors: b'modern', b'including', b'history', b'include', b'based', b'list', b'works', b'of',\n",
            "\"b'physics'\" nearest neighbors: b'modern', b'history', b'book', b'on', b'from', b'further', b'between', b'non',\n",
            "\"b'finance'\" nearest neighbors: b'major', b'or', b'single', b'even', b'for', b'each', b'an', b'development',\n",
            "\"b'language'\" nearest neighbors: b'modern', b'include', b'other', b'also', b'form', b'most', b'word', b'common',\n",
            "\"b'eat'\" nearest neighbors: b'languages', b'often', b'short', b'earth', b'today', b'example', b'many', b'some',\n",
            "\"b'happy'\" nearest neighbors: b'religious', b'played', b'due', b'show', b'through', b'according', b'making', b'young',\n",
            "Step 1410000, Average Loss= 6.2128\n",
            "Step 1420000, Average Loss= 6.2338\n",
            "Step 1430000, Average Loss= 6.2025\n",
            "Step 1440000, Average Loss= 6.1835\n",
            "Step 1450000, Average Loss= 6.1229\n",
            "Step 1460000, Average Loss= 5.9473\n",
            "Step 1470000, Average Loss= 6.2267\n",
            "Step 1480000, Average Loss= 6.1277\n",
            "Step 1490000, Average Loss= 6.2659\n",
            "Step 1500000, Average Loss= 6.0488\n",
            "Step 1510000, Average Loss= 6.1324\n",
            "Step 1520000, Average Loss= 6.2023\n",
            "Step 1530000, Average Loss= 6.1298\n",
            "Step 1540000, Average Loss= 6.0776\n",
            "Step 1550000, Average Loss= 6.1721\n",
            "Step 1560000, Average Loss= 6.1052\n",
            "Step 1570000, Average Loss= 6.1249\n",
            "Step 1580000, Average Loss= 6.1351\n",
            "Step 1590000, Average Loss= 6.0814\n",
            "Step 1600000, Average Loss= 6.1513\n",
            "Evaluation...\n",
            "\"b'science'\" nearest neighbors: b'history', b'including', b'community', b'include', b'modern', b'see', b'culture', b'based',\n",
            "\"b'physics'\" nearest neighbors: b'modern', b'its', b'and', b'article', b'traditional', b'along', b'the', b'non',\n",
            "\"b'finance'\" nearest neighbors: b'major', b'single', b'its', b'development', b'an', b'together', b'all', b'from',\n",
            "\"b'language'\" nearest neighbors: b'modern', b'languages', b'include', b'word', b'such', b'other', b'various', b'culture',\n",
            "\"b'eat'\" nearest neighbors: b'languages', b'such', b'many', b'have', b'forms', b'known', b'found', b'most',\n",
            "\"b'happy'\" nearest neighbors: b'religious', b'with', b'little', b'an', b'etc', b'view', b'show', b'due',\n",
            "Step 1610000, Average Loss= 6.1170\n",
            "Step 1620000, Average Loss= 6.0895\n",
            "Step 1630000, Average Loss= 6.1189\n",
            "Step 1640000, Average Loss= 6.0158\n",
            "Step 1650000, Average Loss= 6.0492\n",
            "Step 1660000, Average Loss= 6.0894\n",
            "Step 1670000, Average Loss= 5.9700\n",
            "Step 1680000, Average Loss= 6.1038\n",
            "Step 1690000, Average Loss= 5.9871\n",
            "Step 1700000, Average Loss= 6.0763\n",
            "Step 1710000, Average Loss= 5.9316\n",
            "Step 1720000, Average Loss= 5.8109\n",
            "Step 1730000, Average Loss= 5.9121\n",
            "Step 1740000, Average Loss= 6.0755\n",
            "Step 1750000, Average Loss= 5.9762\n",
            "Step 1760000, Average Loss= 5.9752\n",
            "Step 1770000, Average Loss= 5.9430\n",
            "Step 1780000, Average Loss= 5.9687\n",
            "Step 1790000, Average Loss= 6.0327\n",
            "Step 1800000, Average Loss= 5.9340\n",
            "Evaluation...\n",
            "\"b'science'\" nearest neighbors: b'history', b'including', b'modern', b'art', b'list', b'include', b'computer', b'development',\n",
            "\"b'physics'\" nearest neighbors: b'modern', b'traditional', b'includes', b'along', b'today', b'further', b'research', b'various',\n",
            "\"b'finance'\" nearest neighbors: b'major', b'based', b'development', b'together', b'different', b'terms', b'single', b'another',\n",
            "\"b'language'\" nearest neighbors: b'modern', b'languages', b'word', b'include', b'most', b'various', b'common', b'culture',\n",
            "\"b'eat'\" nearest neighbors: b'such', b'known', b'languages', b'today', b'other', b'as', b'well', b'actually',\n",
            "\"b'happy'\" nearest neighbors: b'religious', b'little', b'young', b'show', b'between', b'design', b'result', b'water',\n",
            "Step 1810000, Average Loss= 5.9508\n",
            "Step 1820000, Average Loss= 6.0256\n",
            "Step 1830000, Average Loss= 5.9230\n",
            "Step 1840000, Average Loss= 5.9972\n",
            "Step 1850000, Average Loss= 5.9049\n",
            "Step 1860000, Average Loss= 5.9712\n",
            "Step 1870000, Average Loss= 6.0230\n",
            "Step 1880000, Average Loss= 5.9308\n",
            "Step 1890000, Average Loss= 5.9601\n",
            "Step 1900000, Average Loss= 5.8966\n",
            "Step 1910000, Average Loss= 5.9283\n",
            "Step 1920000, Average Loss= 5.8651\n",
            "Step 1930000, Average Loss= 5.9585\n",
            "Step 1940000, Average Loss= 5.8659\n",
            "Step 1950000, Average Loss= 5.9120\n",
            "Step 1960000, Average Loss= 5.8963\n",
            "Step 1970000, Average Loss= 5.8696\n",
            "Step 1980000, Average Loss= 5.8436\n",
            "Step 1990000, Average Loss= 5.6215\n",
            "Step 2000000, Average Loss= 5.8891\n",
            "Evaluation...\n",
            "\"b'science'\" nearest neighbors: b'art', b'technology', b'research', b'list', b'based', b'development', b'community', b'modern',\n",
            "\"b'physics'\" nearest neighbors: b'modern', b'includes', b'traditional', b'along', b'research', b'classical', b'free', b'various',\n",
            "\"b'finance'\" nearest neighbors: b'major', b'terms', b'personal', b'development', b'or', b'particular', b'instead', b'single',\n",
            "\"b'language'\" nearest neighbors: b'modern', b'languages', b'word', b'include', b'culture', b'traditional', b'various', b'group',\n",
            "\"b'eat'\" nearest neighbors: b'today', b'such', b'actually', b'food', b'well', b'some', b'usually', b'languages',\n",
            "\"b'happy'\" nearest neighbors: b'little', b'religious', b'design', b'a', b'or', b'result', b'etc', b'their',\n",
            "Step 2010000, Average Loss= 5.8390\n",
            "Step 2020000, Average Loss= 5.9548\n",
            "Step 2030000, Average Loss= 5.7751\n",
            "Step 2040000, Average Loss= 5.8323\n",
            "Step 2050000, Average Loss= 5.8992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jUnBwKUTMPgS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}